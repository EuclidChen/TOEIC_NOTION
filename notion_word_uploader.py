# âœ… è‡ªå‹•å–®å­—è¨˜æ†¶ç”Ÿæˆ + ä¸Šå‚³ Notion å·¥å…·ï¼ˆGPT 4 æ–°ç‰ˆ APIï¼‰+ Web UI ä»‹é¢
# æ”¾ç½®æ–¼ï¼šC:\Users\Euclid Chen\Documents\privacy\TOEIC

import os
import pandas as pd
import time
import json
from dotenv import load_dotenv
from notion_client import Client
from openai import OpenAI
import streamlit as st
from datetime import datetime, timedelta, timezone




# è¼‰å…¥ .env ç’°å¢ƒè®Šæ•¸
load_dotenv()

NOTION_TOKEN = os.getenv("NOTION_TOKEN")
DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)
notion = Client(auth=NOTION_TOKEN)

st.set_page_config(page_title="å¤šç›Šå–®å­—ä¸Šå‚³å™¨", layout="wide")
st.title("ğŸ§  å¤šç›Šå–®å­—è¨˜æ†¶ç”Ÿæˆå™¨ + è‡ªå‹•ä¸Šå‚³ Notion")

# ğŸ“… é¡¯ç¤ºä»Šæ—¥æ‡‰è¤‡ç¿’å–®å­—åŠŸèƒ½
if st.button("ğŸ“… ä»Šå¤©è¦è¤‡ç¿’å“ªäº›å–®å­—ï¼Ÿ"):
    today = datetime.now(timezone.utc)  # æ”¹ç‚º offset-aware datetime
    notion_data = notion.databases.query(database_id=DATABASE_ID)
    results = notion_data["results"]

    words_due = []

    for page in results:
        props = page["properties"]
        title = props["Word"]["title"][0]["text"]["content"] if props["Word"]["title"] else ""
        review_tags = [tag["name"] for tag in props["Review"]["multi_select"]]
        created_time = datetime.fromisoformat(page["created_time"].replace("Z", "+00:00"))
        day_diff = (today - created_time).days
        tag_today = f"D{day_diff}"
        if tag_today in review_tags:
            words_due.append((title, tag_today))

    if words_due:
        st.subheader("âœ… ä»Šæ—¥æ‡‰è¤‡ç¿’ï¼š")
        for word, tag in words_due:
            st.markdown(f"- **{word}**ï¼ˆæ¨™è¨»ï¼š{tag}ï¼‰")
    else:
        st.info("ä»Šå¤©æ²’æœ‰ç¬¦åˆçš„è¤‡ç¿’å–®å­— âœ…")

words_input = st.text_area("è«‹è¼¸å…¥å–®å­—ï¼ˆå¯å¤šå€‹ï¼Œè«‹ç”¨é€—è™Ÿæˆ–æ›è¡Œåˆ†éš”ï¼‰")
submit = st.button("ğŸš€ ç”¢ç”Ÿä¸¦ä¸Šå‚³")

status_container = st.container()
success_list = []
fail_list = []

if submit and words_input:
    vocab_list = [w.strip() for w in words_input.replace('\n', ',').split(',') if w.strip()]

    def generate_word_info(word):
        prompt = f"""
è«‹é‡å°å¤šç›Šå–®å­—ã€Œ{word}ã€ç”¢å‡ºä»¥ä¸‹å…§å®¹ï¼š

1. è©æ€§èˆ‡ä¸­æ–‡æ„æ€ï¼ˆä¾‹å¦‚ï¼šn. é£›æ©Ÿï¼‰
2. è¨˜æ†¶éŒ¨é»ï¼ˆç”¨è¯æƒ³ã€æ‹†å­—ã€è«§éŸ³ã€æƒ…å¢ƒç­‰æ–¹å¼è®“äººè¨˜å¾—å–®å­—ï¼‰
3. èˆ‡ TOEIC ç›¸é—œçš„ä¾‹å¥ï¼ˆ3 å¥ï¼Œæ¯å¥é™„ä¸Šä¸­æ–‡ç¿»è­¯ï¼‰
4. ä¸€å€‹ YouTube æ­Œè©æˆ–å½±ç‰‡é€£çµï¼ˆèˆ‡æ­¤å–®å­—æœ‰æƒ…å¢ƒé€£çµï¼‰
5. èªæ„ç¶²è·¯ï¼ˆåˆ—å‡º 2~3 å€‹åŒç¾©è©ï¼‹èªªæ˜å·®ç•°ï¼‹å¸¸è¦‹è©çµ„ï¼‹3å€‹æ­é…ä¾‹å¥ï¼‰

è«‹ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼Œæ³¨æ„æ‰€æœ‰æ¬„ä½çš„å…§å®¹å¿…é ˆç‚ºç´”æ–‡å­—æ ¼å¼ï¼š
{{
  "Word": "{word}",
  "Part of Speech": "",
  "Chinese": "",
  "Anchor": "",
  "Video": "",
  "Semantic": "",
  "Example1": "",
  "Example2": "",
  "Example3": "",
  "Review": "D2,D4,D7,D14,D30"
}}
"""

        try:
            res = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä½å¤šç›Šå–®å­—è¨˜æ†¶è¨­è¨ˆå°ˆå®¶ï¼Œæ“…é•·å¹«åŠ©è¨˜æ†¶èˆ‡èªæ„è¯æƒ³ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
            )
            content = res.choices[0].message.content
            result = json.loads(content.strip())
            return result
        except Exception as e:
            fail_list.append((word, f"GPT ç”Ÿæˆå¤±æ•—ï¼š{e}"))
            return None

    data = []
    for word in vocab_list:
        status_container.info(f"ğŸ” æ­£åœ¨è™•ç†ï¼š{word}")
        result = generate_word_info(word)
        if result:
            data.append(result)
        time.sleep(2)

    if data:
        df = pd.DataFrame(data)
        timestamp = datetime.now().strftime("%Y%m%d%H%M")
        output_dir = os.path.join(os.path.dirname(__file__), "outputs")
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"output_today_{timestamp}.csv")
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        status_container.success(f"ğŸ“ CSV å·²å„²å­˜ï¼š{output_path}")

        for row in data:
            try:
                notion.pages.create(
                    parent={"database_id": DATABASE_ID},
                    properties={
                        "Word": {"title": [{"text": {"content": str(row['Word'])}}]},
                        "Part of Speech": {"select": {"name": str(row['Part of Speech'])}},
                        "Chinese": {"rich_text": [{"text": {"content": str(row['Chinese'])}}]},
                        "Anchor": {"rich_text": [{"text": {"content": str(row['Anchor'])}}]},
                        "Video": {"url": str(row['Video'])},
                        "Semantic": {"rich_text": [{"text": {"content": str(row['Semantic']).replace('. ', '.\n')}}]},
                        "Example 1": {"rich_text": [{"text": {"content": str(row['Example1'])}}]},
                        "Example 2": {"rich_text": [{"text": {"content": str(row['Example2'])}}]},
                        "Example 3": {"rich_text": [{"text": {"content": str(row['Example3'])}}]},
                        "Review": {"multi_select": [{"name": tag.strip()} for tag in row['Review'].split(',')]}
                    }
                )
                success_list.append(row['Word'])
            except Exception as e:
                fail_list.append((row['Word'], f"ä¸Šå‚³å¤±æ•—ï¼š{e}"))

        with st.expander("ğŸ“ˆ åŸ·è¡Œçµæœç¸½çµ", expanded=True):
            st.markdown(f"### âœ… æˆåŠŸä¸Šå‚³ï¼š{len(success_list)} ç­†")
            for word in success_list:
                st.markdown(f"- {word}")

            if fail_list:
                st.markdown(f"### âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{len(fail_list)} ç­†")
                for word, err in fail_list:
                    st.markdown(f"- {word}ï¼š{err}")
    else:
        st.warning("âš ï¸ æ²’æœ‰ä»»ä½•è³‡æ–™ç”¢å‡ºï¼")
