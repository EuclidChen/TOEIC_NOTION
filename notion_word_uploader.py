import os
import pandas as pd
import time
import json
from dotenv import load_dotenv
from notion_client import Client
from openai import OpenAI
import streamlit as st
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
import matplotlib.pyplot as plt

# è¼‰å…¥ .env ç’°å¢ƒè®Šæ•¸
load_dotenv()

NOTION_TOKEN = os.getenv("NOTION_TOKEN")
DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)
notion = Client(auth=NOTION_TOKEN)

st.set_page_config(page_title="å¤šç›Šå–®å­—å·¥å…·æ•´åˆ", layout="wide")
st.title("ğŸ§  å¤šç›Šå–®å­—è¨˜æ†¶å·¥å…·æ•´åˆå¹³å°")

# å»ºç«‹åˆ†é 
tabs = st.tabs(["ğŸ“¤ å–®å­—ä¸Šå‚³", "ğŸ“… ä»Šæ—¥è¤‡ç¿’", "ğŸ“Š å–®å­—çµ±è¨ˆ", "ğŸ“ˆ æ¯æ—¥æ–°å¢å–®å­—"])

#ğŸ“¤ å–®å­—ä¸Šå‚³
with tabs[0]:
    st.header("ğŸ“¤ å–®å­—ä¸Šå‚³åŠŸèƒ½")

    # -- æ–°ï¼šä½¿ç”¨ st.form(clear_on_submit=True) --
    with st.form("upload_form", clear_on_submit=True):
        words_input = st.text_area(
            "è«‹è¼¸å…¥å–®å­—ï¼ˆå¯å¤šå€‹ï¼Œè«‹ç”¨é€—è™Ÿæˆ–æ›è¡Œåˆ†éš”ï¼‰",
            key="words_input",
            placeholder="ä¾‹å¦‚ï¼šallocate, consolidate, substantial",
            height=150
        )
        submit = st.form_submit_button("ğŸš€ ç”¢ç”Ÿä¸¦ä¸Šå‚³")

    status_container = st.container()          # å‹•æ…‹è¨Šæ¯é¡¯ç¤º
    success_list, fail_list = [], []           # æˆåŠŸï¼å¤±æ•—ç´€éŒ„

    # åªæœ‰æŒ‰ä¸‹æŒ‰éˆ•ä¸”è¼¸å…¥éç©ºæ™‚æ‰ç¹¼çºŒ
    if submit and words_input:
        vocab_list = [w.strip()
                      for w in words_input.replace("\n", ",").split(",")
                      if w.strip()]

        # ---------- GPT ç”¢ç”Ÿè³‡è¨Š ----------
        def generate_word_info(word: str) -> dict | None:
            prompt = f"""
è«‹é‡å°å¤šç›Šå–®å­—ã€Œ{word}ã€ç”¢å‡ºä»¥ä¸‹å…§å®¹ï¼š

1. è©æ€§èˆ‡ä¸­æ–‡æ„æ€ï¼ˆä¾‹å¦‚ï¼šn. é£›æ©Ÿï¼‰
2. è¨˜æ†¶éŒ¨é»ï¼ˆç”¨è¯æƒ³ã€æ‹†å­—ã€è«§éŸ³ã€æƒ…å¢ƒç­‰æ–¹å¼è®“äººè¨˜å¾—å–®å­—ï¼‰
3. èˆ‡ TOEIC ç›¸é—œçš„ä¾‹å¥ï¼ˆ3 å¥ï¼Œæ¯å¥é™„ä¸Šä¸­æ–‡ç¿»è­¯ï¼‰
4. ä¸€å€‹ YouTube æ­Œè©æˆ–å½±ç‰‡é€£çµï¼ˆèˆ‡æ­¤å–®å­—æœ‰æƒ…å¢ƒé€£çµï¼‰
5. èªæ„ç¶²è·¯ï¼ˆåˆ—å‡º 2~3 å€‹åŒç¾©è©ï¼‹èªªæ˜å·®ç•°ï¼‹å¸¸è¦‹è©çµ„ï¼‹3å€‹æ­é…ä¾‹å¥ï¼‰

è«‹ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼Œæ‰€æœ‰æ¬„ä½å¿…é ˆç‚ºç´”æ–‡å­—ï¼š
{{
  "Word": "{word}",
  "Part of Speech": "",
  "Chinese": "",
  "Anchor": "",
  "Video": "",
  "Semantic": "",
  "Example1": "",
  "Example2": "",
  "Example3": "",
  "Review": "D2,D4,D7,D14,D30"
}}
"""
            try:
                res = client.chat.completions.create(
                    model="gpt-4",             # é€Ÿåº¦è¼ƒå¿«ï¼Œè¦–éœ€è¦æ”¹å› gpt-4
                    messages=[
                        {"role": "system",
                         "content": "ä½ æ˜¯å¤šç›Šå–®å­—è¨˜æ†¶è¨­è¨ˆå°ˆå®¶ï¼Œæ“…é•·å¹«åŠ©è¨˜æ†¶èˆ‡èªæ„è¯æƒ³ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                )
                content = res.choices[0].message.content
                return json.loads(content.strip())
            except Exception as e:
                fail_list.append((word, f"GPT ç”Ÿæˆå¤±æ•—ï¼š{e}"))
                return None
        # ------------------------------------

        data = []
        for word in vocab_list:
            status_container.info(f"ğŸ” æ­£åœ¨è™•ç†ï¼š{word}")
            result = generate_word_info(word)
            if result:
                data.append(result)
            time.sleep(1.5)   # é¿å…éåº¦å‘¼å«

        # ---------- è‹¥æœ‰æˆåŠŸè³‡æ–™ ----------
        if data:
            df = pd.DataFrame(data)

            # 1) å…ˆå­˜æˆ CSV
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            output_dir = os.path.join(os.path.dirname(__file__), "outputs")
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir,
                                       f"output_today_{timestamp}.csv")
            df.to_csv(output_path, index=False, encoding="utf-8-sig")
            status_container.success(f"ğŸ“ CSV å·²å„²å­˜ï¼š{output_path}")

            # 2) é€ç­†å¯«å…¥ Notion
            for row in data:
                try:
                    notion.pages.create(
                        parent={"database_id": DATABASE_ID},
                        properties={
                            "Word": {
                                "title": [{"text": {"content": row["Word"]}}]},
                            "Part of Speech": {
                                "select": {"name": row["Part of Speech"]}},
                            "Chinese": {
                                "rich_text": [{"text": {
                                    "content": row["Chinese"]}}]},
                            "Anchor": {
                                "rich_text": [{"text": {
                                    "content": row["Anchor"]}}]},
                            "Video": {"url": row["Video"]},
                            "Semantic": {
                                "rich_text": [{"text": {
                                    "content": row["Semantic"].replace(". ", ".\n")}}]},
                            "Example 1": {
                                "rich_text": [{"text": {
                                    "content": row["Example1"]}}]},
                            "Example 2": {
                                "rich_text": [{"text": {
                                    "content": row["Example2"]}}]},
                            "Example 3": {
                                "rich_text": [{"text": {
                                    "content": row["Example3"]}}]},
                            "Review": {
                                "multi_select": [{"name": t.strip()}
                                                 for t in row["Review"].split(",")]}
                        }
                    )
                    success_list.append(row["Word"])
                except Exception as e:
                    fail_list.append((row["Word"], f"ä¸Šå‚³å¤±æ•—ï¼š{e}"))

            # 3) çµæœæ‘˜è¦
            with st.expander("ğŸ“ˆ åŸ·è¡Œçµæœç¸½çµ", expanded=True):
                st.markdown(f"### âœ… æˆåŠŸä¸Šå‚³ï¼š{len(success_list)} ç­†")
                for w in success_list:
                    st.markdown(f"- {w}")

                if fail_list:
                    st.markdown(f"### âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{len(fail_list)} ç­†")
                    for w, err in fail_list:
                        st.markdown(f"- {w}ï¼š{err}")
        else:
            st.warning("âš ï¸ æ²’æœ‰ä»»ä½•è³‡æ–™ç”¢å‡ºï¼")


# ğŸ“… ä»Šæ—¥è¤‡ç¿’
with tabs[1]:
    st.header("ğŸ“… ä»Šæ—¥æ‡‰è¤‡ç¿’å–®å­—")
    if st.button("ğŸ“… ä»Šå¤©è¦è¤‡ç¿’å“ªäº›å–®å­—ï¼Ÿ"):
        tz_tw = ZoneInfo("Asia/Taipei")
        today = datetime.now(tz=tz_tw).replace(hour=0, minute=0, second=0, microsecond=0)
        notion_data = notion.databases.query(database_id=DATABASE_ID)
        results = notion_data["results"]

        words_due = []

        for page in results:
            props = page["properties"]
            title = props["Word"]["title"][0]["text"]["content"] if props["Word"]["title"] else ""
            review_tags = [tag["name"] for tag in props["Review"]["multi_select"]]
            created_time = datetime.fromisoformat(page["created_time"].replace("Z", "+00:00")).astimezone(tz_tw).replace(hour=0, minute=0, second=0, microsecond=0)
            day_diff = (today - created_time).days
            tag_today = f"D{day_diff}"
            if tag_today in review_tags:
                words_due.append((title, tag_today))

        if words_due:
            st.subheader("âœ… ä»Šæ—¥æ‡‰è¤‡ç¿’ï¼š")
            for word, tag in words_due:
                st.markdown(f"- **{word}**ï¼ˆæ¨™è¨»ï¼š{tag}ï¼‰")
        else:
            st.info("ä»Šå¤©æ²’æœ‰ç¬¦åˆçš„è¤‡ç¿’å–®å­— âœ…")

# ğŸ“Š å–®å­—çµ±è¨ˆçœ‹æ¿
with tabs[2]:
    st.header("ğŸ“Š å–®å­—çµ±è¨ˆçœ‹æ¿")
    try:
        notion_data = notion.databases.query(database_id=DATABASE_ID)
        results = notion_data["results"]
        st.markdown(f"**ç›®å‰ç¸½å…±æœ‰ {len(results)} ç­†å–®å­—**")
        tag_counter = {"D2": 0, "D4": 0, "D7": 0, "D14": 0, "D30": 0}
        for page in results:
            tags = page["properties"]["Review"]["multi_select"]
            for tag in tags:
                if tag["name"] in tag_counter:
                    tag_counter[tag["name"]] += 1
        for tag, count in tag_counter.items():
            st.markdown(f"- {tag}ï¼š{count} ç­†")
    except Exception as e:
        st.error(f"çµ±è¨ˆè³‡æ–™è®€å–å¤±æ•—ï¼š{e}")

# ğŸ“ˆ æ¯æ—¥æ–°å¢å–®å­—
with tabs[3]:
    st.header("ğŸ“ˆ æ¯æ—¥æ–°å¢å–®å­—æ•¸é‡")
    try:
        tz_tw = ZoneInfo("Asia/Taipei")
        results = notion.databases.query(database_id=DATABASE_ID)["results"]
        date_counts = {}
        for page in results:
            created_time = datetime.fromisoformat(page["created_time"].replace("Z", "+00:00"))
            local_date = created_time.astimezone(tz_tw).date()
            date_counts[local_date] = date_counts.get(local_date, 0) + 1

        df = pd.DataFrame(list(date_counts.items()), columns=["Date", "Count"]).sort_values("Date")
        df["Date"] = pd.to_datetime(df["Date"])

        st.bar_chart(data=df, x="Date", y="Count")
        st.success("âœ… æˆåŠŸç”¢å‡ºæ¯æ—¥æ–°å¢çµ±è¨ˆåœ–")
    except Exception as e:
        st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
